{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5602e8",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# PASO 1: CONFIGURACIÓN DEL ENTORNO E IMPORTACIÓN DE LIBRERÍAS\n",
    "# =============================================================================\n",
    "# Importamos las librerías fundamentales para el análisis y modelado.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Librerías de Scikit-Learn para preprocesamiento y modelado\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Modelos de clasificación\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Métricas de evaluación\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Librerías importadas correctamente.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4e329",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 2: CARGA DE DATOS\n",
    "# =============================================================================\n",
    "# Para este ejemplo, usaremos el dataset del Titanic, que está disponible en Seaborn.\n",
    "# Este dataset es ideal para practicar porque contiene valores nulos y tipos de datos mixtos.\n",
    "print(\"\\nCargando el dataset del Titanic...\")\n",
    "df = sns.load_dataset('titanic')\n",
    "print(\"Dataset cargado. Aquí están las primeras 5 filas:\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323caae8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 3: ANÁLISIS EXPLORATORIO DE DATOS (EDA)\n",
    "# =============================================================================\n",
    "# El EDA es fundamental para entender la estructura, la calidad y las relaciones en los datos.\n",
    "\n",
    "print(\"\\n--- Iniciando Análisis Exploratorio de Datos (EDA) ---\")\n",
    "\n",
    "# 3.1. Información General del Dataset\n",
    "print(\"\\nInformación general del DataFrame (df.info()):\")\n",
    "df.info()\n",
    "\n",
    "# 3.2. Resumen Estadístico de Variables Numéricas\n",
    "print(\"\\nResumen estadístico de variables numéricas (df.describe()):\")\n",
    "print(df.describe())\n",
    "\n",
    "# 3.3. Análisis de Valores Nulos\n",
    "print(\"\\nConteo de valores nulos por columna:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Visualización de valores nulos\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title('Mapa de Calor de Valores Nulos')\n",
    "plt.show()\n",
    "\n",
    "# 3.4. Análisis Univariado (Análisis de variables individuales)\n",
    "print(\"\\nRealizando análisis univariado...\")\n",
    "\n",
    "# Supervivencia (nuestra variable objetivo)\n",
    "sns.countplot(x='survived', data=df)\n",
    "plt.title('Distribución de Supervivencia (0 = No, 1 = Sí)')\n",
    "plt.show()\n",
    "\n",
    "# Clases de pasajeros\n",
    "sns.countplot(x='pclass', data=df)\n",
    "plt.title('Distribución de Clases de Pasajeros')\n",
    "plt.show()\n",
    "\n",
    "# Género\n",
    "sns.countplot(x='sex', data=df)\n",
    "plt.title('Distribución por Género')\n",
    "plt.show()\n",
    "\n",
    "# Distribución de Edad\n",
    "sns.histplot(df['age'].dropna(), kde=True, bins=30)\n",
    "plt.title('Distribución de Edades')\n",
    "plt.show()\n",
    "\n",
    "# 3.5. Análisis Bivariado (Relación entre dos variables)\n",
    "print(\"\\nRealizando análisis bivariado...\")\n",
    "\n",
    "# Supervivencia vs. Clase\n",
    "sns.countplot(x='survived', hue='pclass', data=df)\n",
    "plt.title('Supervivencia por Clase de Pasajero')\n",
    "plt.show()\n",
    "\n",
    "# Supervivencia vs. Género\n",
    "sns.countplot(x='survived', hue='sex', data=df)\n",
    "plt.title('Supervivencia por Género')\n",
    "plt.show()\n",
    "\n",
    "# Supervivencia vs. Edad\n",
    "g = sns.FacetGrid(df, col='survived', height=5)\n",
    "g.map(sns.histplot, 'age', bins=20, kde=False)\n",
    "plt.suptitle('Distribución de Edad por Supervivencia', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# 3.6. Matriz de Correlación para variables numéricas\n",
    "print(\"\\nCalculando matriz de correlación...\")\n",
    "# Creamos una copia para el heatmap, seleccionando solo columnas numéricas\n",
    "numeric_cols = df.select_dtypes(include=np.number)\n",
    "correlation_matrix = numeric_cols.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Matriz de Correlación de Variables Numéricas')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4bddd4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 4: PREPROCESAMIENTO DE DATOS E INGENIERÍA DE CARACTERÍSTICAS\n",
    "# =============================================================================\n",
    "print(\"\\n--- Iniciando Preprocesamiento de Datos ---\")\n",
    "\n",
    "# Eliminamos columnas que no aportan información útil o son difíciles de tratar\n",
    "# 'deck' tiene demasiados nulos, 'embark_town' es redundante con 'embarked', etc.\n",
    "df_processed = df.drop(['deck', 'embark_town', 'alive', 'who', 'adult_male', 'class'], axis=1)\n",
    "\n",
    "# Separamos la variable objetivo (y) de las predictoras (X)\n",
    "X = df_processed.drop('survived', axis=1)\n",
    "y = df_processed['survived']\n",
    "\n",
    "# Identificamos columnas numéricas y categóricas\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object', 'category', 'bool']).columns\n",
    "\n",
    "print(f\"Columnas numéricas: {list(numerical_features)}\")\n",
    "print(f\"Columnas categóricas: {list(categorical_features)}\")\n",
    "\n",
    "# Creamos pipelines de preprocesamiento para cada tipo de dato\n",
    "# Para datos numéricos: imputamos la media y luego escalamos.\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Para datos categóricos: imputamos la moda y luego aplicamos One-Hot Encoding.\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Usamos ColumnTransformer para aplicar los pipelines a las columnas correctas\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Dividimos los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nDatos divididos: {len(X_train)} para entrenamiento, {len(X_test)} para prueba.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d7375",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 5: ENTRENAMIENTO Y EVALUACIÓN DE MODELOS\n",
    "# =============================================================================\n",
    "print(\"\\n--- Entrenando y Evaluando Modelos ---\")\n",
    "\n",
    "# Definimos los modelos que vamos a probar\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Iteramos sobre cada modelo para entrenarlo y evaluarlo\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Creamos un pipeline completo que incluye preprocesamiento y el modelo\n",
    "    model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                     ('classifier', model)])\n",
    "\n",
    "    # Entrenamos el pipeline\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Realizamos predicciones\n",
    "    y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "    # Calculamos la precisión\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "\n",
    "    # Imprimimos los resultados\n",
    "    print(f\"\\n--- Resultados para: {name} ---\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Matriz de Confusión - {name}')\n",
    "    plt.xlabel('Predicho')\n",
    "    plt.ylabel('Verdadero')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b2b6a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 6: COMPARACIÓN DE MODELOS\n",
    "# =============================================================================\n",
    "print(\"\\n--- Comparación Final de Modelos ---\")\n",
    "results_df = pd.DataFrame(list(results.items()), columns=['Model', 'Accuracy']).sort_values(by='Accuracy', ascending=False)\n",
    "print(results_df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Accuracy', y='Model', data=results_df)\n",
    "plt.title('Comparación de la Precisión de los Modelos')\n",
    "plt.xlim(0.7, 0.9)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAnálisis finalizado.\")\n",
    "print(\"El modelo con mejor rendimiento inicial es el Gradient Boosting.\")\n",
    "print(\"El siguiente paso podría ser la optimización de hiperparámetros para este modelo.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
