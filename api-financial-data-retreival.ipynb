{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "af1c600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from IPython.display import display\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 1. CONFIGURACIÓN Y LÍMITES (Ajustado a 25 llamadas/día)\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# API_KEY = \"K4MT78V60FZAHTXR\"  \n",
    "# API_KEY = \"L6L6UX08IX4UCMLN\"\n",
    "# API_KEY = \"WW3U7VK6D1KSVDPF\"\n",
    "API_KEY = \"AQS81ZD4KI6WC0UB\"\n",
    "\n",
    "BASE_URL_AV = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "\n",
    "TICKERS = []\n",
    "\n",
    "TICKERS_RESTANTES = ['WPP', 'FULT', 'OII', 'FSTR', 'FHN', 'BRSL', 'MGPI', 'MLKN', 'COHR', 'IDCC', 'TEX', 'HCSG', 'LYTS', 'JJSF', 'LSCC', 'SENEB', 'RGEN', 'COKE', 'CUZ', 'CULP', 'DGII', 'SUNE', 'CRUS', 'MCY', 'MSEX', 'MERC', 'MSA', 'SIGI', 'HNI', 'CLDX', 'UFCS', 'RGR', 'CWT', 'TRST', 'HTLD', 'JOUT', 'DAIO', 'KTCC', 'MOD', 'MGEE', 'SHYF', 'SBCF', 'STKL', 'LAKE', 'MGRC', 'PATK', 'VLGEA', 'AXGN', 'ITIC', 'ENB', 'TRNS', 'CSPI', 'UMBF', 'ASRV', 'INDB', 'CSWC', 'STC', 'OFG', 'MCS', 'ELSE', 'TTI', 'CDE', 'RIO', 'EXPO', 'DY', 'ESE', 'DDD', 'CATY', 'AVNW', 'NBR', 'DORM', 'XRAY', 'GFI', 'HAE', 'IONS', 'LCUT', 'CRVL', 'ATGE', 'DIN', 'ALKS', 'MNRO', 'PRGS', 'MTG', 'IPAR', 'KEX', 'EZPW', 'MGIC', 'PRA', 'BOKF', 'FIZZ', 'TDS', 'KMPR', 'NPKI', 'NHI', 'RHP', 'TTE', 'WNC', 'ATNI', 'HMN', 'VICR', 'WRLD', 'ACNT', 'JBSS', 'OI', 'AMSC', 'TTEK', 'PRGO', 'CDP', 'GBCI', 'M', 'CMTL', 'MOG-A', 'GVA', 'SCHL', 'DXLG', 'DJCO', 'KEQU', 'FLXS', 'HVT-A', 'FCNCA', 'BH', 'FBP', 'ALNT', 'MAYS', 'MMSI', 'ONTO', 'MTRX', 'MRTN', 'CRT', 'MODG', 'STAA', 'AN', 'TNC', 'THFF', 'TRMK', 'NEON', 'JOE', 'VIRC', 'AD', 'NBTB', 'VAL', 'HEI', 'SMTC', 'DXR', 'SGC', 'SCL', 'HOV', 'UTL', 'CVBF', 'EGP', 'OLP', 'UG', 'OSUR', 'IIIN', 'ELME', 'DIOD', 'DINO', 'EML', 'WSO-B', 'TRC', 'WABC', 'MIDD', 'USAU', 'RDI', 'GHM', 'WEN', 'CRD-A', 'MBOT', 'ICUI', 'ASUR', 'KOPN', 'AGCO', 'STBA', 'OFIX', 'RNST', 'BKE', 'ARCB', 'MEOH', 'KSS', 'BLFS', 'USPH', 'FC', 'CACC', 'FCF', 'COLB', 'DDC', 'OHI', 'FCFS', 'LTC', 'FUNC', 'CTO', 'EQS', 'CAKE', 'LFUS', 'ASGN', 'BLX', 'CECO', 'ERNA', 'MTX', 'RDN', 'SFNC', 'KAI', 'LGND', 'FBNC', 'GPK', 'AAON', 'SM', 'ULBI', 'RRC', 'IAC', 'EGY', 'WOLF', 'PEBO', 'AORT', 'UEIC', 'CVU', 'CRH', 'CHCO', 'CPSS', 'ELS', 'NATH', 'STRS', 'EDUC', 'FNB', 'WT', 'LSTR', 'TATT', 'SCVL', 'ETD', 'GTN-A', 'LDWY', 'ALG', 'SNT', 'GILT', 'CHDN', 'AMRN', 'TCBK', 'ATR', 'ANIK', 'NTZ', 'AZN', 'CTBI', 'TWI', 'GABC', 'AKR', 'HR', 'RIG', 'SKT', 'PZZA', 'THRM', 'BBSI', 'SANM', 'NSYS', 'YPF', 'CENT', 'MCRI', 'INOD', 'BWA', 'OPY', 'FLL', 'BFS', 'WINA', 'FTEK', 'NTIC', 'KOF', 'SQM', 'CASH', 'TLF', 'MOV', 'QCRH', 'SGA', 'BYD', 'LXP', 'NRIM', 'FONR', 'HWKN', 'NAII', 'FFIN', 'PTEN', 'HHS', 'ROCK', 'ITRI', 'CRMT', 'URBN', 'CNTY', 'UFPI', 'AEC', 'FWRD', 'FLG', 'BDC', 'BBAR', 'UMH', 'SUI', 'SHOO', 'TV', 'UFPT', 'REFR', 'MED', 'NOK', 'ITI', 'HAIN', 'RGCO', 'DAKT', 'NEOG', 'RYN', 'BZH', 'BCRX', 'ACHC', 'NNBR', 'FLEX', 'SCSC', 'MPAA', 'LARK', 'EXP', 'AEO', 'ADC', 'NKTR', 'SYPR', 'ABCB', 'PENN', 'SSD', 'HIW', 'IMAX', 'FR', 'AKO-A', 'GBX', 'AIV', 'MLR', 'MATW', 'VRE', 'PNBK', 'DAR', 'SIRI', 'WVVI', 'RS', 'RELX', 'PKX', 'KGC', 'SSYS', 'KNX', 'TSEM', 'KEP', 'BMO', 'CVLG', 'SIG', 'BSAC', 'HDSN', 'UHAL', 'NWBI', 'TGS', 'VECO', 'TEO', 'CYD', 'IRS', 'CIGI', 'MLAB', 'NSIT', 'UPBD', 'TBI', 'USNA', 'AZTA', 'QUAD', 'STRT', 'ACIW']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0f0df812",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filtrar_lista_por_csv(lista, ruta_csv):\n",
    "    df = pd.read_csv(ruta_csv)\n",
    "    if 'ticker' not in df.columns:\n",
    "        raise ValueError(\"El CSV no contiene la columna 'ticker'\")\n",
    "    tickers_csv = set(df['ticker'].dropna().astype(str).str.strip())\n",
    "    lista_filtrada = [item for item in lista if str(item).strip() not in tickers_csv]\n",
    "\n",
    "    return lista_filtrada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "37a9b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 2. FUNCIONES DE UTILIDAD CON CONTROL DE LLAMADAS\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def fetch_data_av(params: Dict[str, str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Realiza una llamada a la API de Alpha Vantage con los parámetros dados.\n",
    "    \"\"\"    \n",
    "    params['apikey'] = API_KEY    \n",
    "    response = requests.get(BASE_URL_AV, params=params)\n",
    "    eliminar = False\n",
    "    print(response.json())\n",
    "\n",
    "    # print(response.json())  # Mostrar la respuesta JSON completa para depuración\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if data:\n",
    "            return data, eliminar\n",
    "        else:\n",
    "            print(f\"⚠️ Datos no encontrados (JSON vacío) para la función: {params.get('function', 'N/A')}\")\n",
    "            eliminar = True\n",
    "            return {}, eliminar\n",
    "    else:\n",
    "        print(f\"❌ Error al solicitar datos (Código: {response.json()}).\")\n",
    "        return {}, eliminar\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3. FUNCIÓN DE EXTRACCIÓN ÚNICA (COMPANY OVERVIEW)\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def get_company_overview_all_metrics(ticker: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extrae TODAS las métricas, ratios y datos de perfil de la función OVERVIEW.\n",
    "    (LLAMA A LA API UNA VEZ)\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Extrayendo TODAS las métricas OVERVIEW para {ticker}---\")\n",
    "    \n",
    "    params_overview = {\n",
    "        \"function\": \"OVERVIEW\",\n",
    "        \"symbol\": ticker\n",
    "    }\n",
    "    data_overview, eliminar = fetch_data_av(params_overview)\n",
    "    \n",
    "    if data_overview and data_overview.get('Symbol') == ticker:\n",
    "        \n",
    "        # Diccionario para almacenar el resultado, incluyendo el ticker\n",
    "        metrics = {'ticker': ticker}\n",
    "        \n",
    "        for key, value in data_overview.items():\n",
    "            if key in ['Error Message', 'Note']:\n",
    "                print(f\"❌ Error o nota en los datos para {ticker}: {value}\")\n",
    "                continue\n",
    "            \n",
    "            # Intentar convertir a numérico si es posible\n",
    "            num_value = pd.to_numeric(value, errors='coerce')\n",
    "            \n",
    "            # Si se pudo convertir, usamos el valor numérico; si no, el valor original (texto)\n",
    "            metrics[key] = num_value if pd.notna(num_value) else value\n",
    "            \n",
    "        return metrics, eliminar\n",
    "    return {}, eliminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7d5892bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Extrayendo TODAS las métricas OVERVIEW para SENEB---\n",
      "{'Information': 'We have detected your API key as AQS81ZD4KI6WC0UB and our standard API rate limit is 25 requests per day. Please subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly remove all daily rate limits.'}\n",
      "❌ No se pudieron extraer métricas para SENEB.\n",
      "\n",
      "❌ No se pudo extraer ninguna métrica de Company Overview.\n"
     ]
    }
   ],
   "source": [
    "all_overviews = []\n",
    "\n",
    "csv_filename = \"company_metrics.csv\"\n",
    "lista_tickers_faltantes = filtrar_lista_por_csv(TICKERS_RESTANTES, csv_filename)\n",
    "# lista_tickers_faltantes = filtrar_lista_por_csv(TICKERS, csv_filename)\n",
    "\n",
    "for ticker in lista_tickers_faltantes:\n",
    "    metrics, eliminar = get_company_overview_all_metrics(ticker)\n",
    "    if eliminar:\n",
    "        lista_tickers_faltantes.remove(ticker)\n",
    "        continue\n",
    "    if metrics:\n",
    "        all_overviews.append(metrics)\n",
    "        lista_tickers_faltantes.remove(ticker)\n",
    "    else:\n",
    "        print(f\"❌ No se pudieron extraer métricas para {ticker}.\")\n",
    "        break\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 5. RESULTADOS (MODIFICADO: AÑADIR A CSV EXISTENTE)\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import os # Necesario para verificar la existencia del archivo\n",
    "\n",
    "if all_overviews:\n",
    "    df_nuevas_overview = pd.DataFrame(all_overviews)\n",
    "    \n",
    "    # 1. Verificar si el archivo ya existe\n",
    "    if os.path.exists(csv_filename):\n",
    "        print(f\"\\n✅ El archivo '{csv_filename}' ya existe. Cargando y añadiendo datos...\")\n",
    "        \n",
    "        # 2. Cargar el CSV existente\n",
    "        df_existente = pd.read_csv(csv_filename)\n",
    "\n",
    "        # Opcional: Evitar duplicados. Filtra las filas nuevas que ya estén en el existente (ej. por 'Symbol' o 'Ticker')\n",
    "        # Asumiendo que 'Symbol' es la columna identificadora.\n",
    "        tickers_existentes = df_existente['ticker'].unique()\n",
    "        df_nuevas_overview = df_nuevas_overview[~df_nuevas_overview['ticker'].isin(tickers_existentes)]\n",
    "        \n",
    "        # 3. Concatenar el DataFrame existente con el nuevo\n",
    "        df_final_combinado = pd.concat([df_existente, df_nuevas_overview], ignore_index=True)\n",
    "        \n",
    "        # Opcional: Eliminar duplicados si es necesario (ej. si se extrajo el mismo ticker dos veces)\n",
    "        df_final_combinado.drop_duplicates(subset=['ticker'], keep='last', inplace=True)\n",
    "        \n",
    "        # 4. Guardar el DataFrame combinado (sobrescribe el archivo con el contenido actualizado)\n",
    "        df_final_combinado.to_csv(csv_filename, index=False)\n",
    "        df_a_mostrar = df_final_combinado # Mostrar el combinado\n",
    "        \n",
    "        print(f\"Se añadieron {len(df_nuevas_overview)} filas nuevas. Total: {len(df_final_combinado)} compañías.\")\n",
    "\n",
    "    else:\n",
    "        # 5. Si el archivo no existe, guárdalo como nuevo\n",
    "        print(f\"\\n⭐ El archivo '{csv_filename}' no existe. Creando nuevo archivo.\")\n",
    "        df_nuevas_overview.to_csv(csv_filename, index=False)\n",
    "        df_a_mostrar = df_nuevas_overview # Mostrar el nuevo\n",
    "    \n",
    "    print(lista_tickers_faltantes)\n",
    "    # Mostrar resultados\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    display(df_a_mostrar)\n",
    "    pd.reset_option('display.max_columns')\n",
    "    print(f\"\\nEl DataFrame final contiene datos de {len(df_a_mostrar)} compañías y {len(df_a_mostrar.columns)} métricas por compañía.\")\n",
    "else:\n",
    "    print(\"\\n❌ No se pudo extraer ninguna métrica de Company Overview.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finan-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
